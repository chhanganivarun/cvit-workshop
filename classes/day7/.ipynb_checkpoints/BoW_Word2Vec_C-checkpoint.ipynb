{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UnNjcTH7wbUP"
   },
   "source": [
    "The objective of this experiment is to learn words with similar or different meanings are equally apart in BoW and semantics or Meaning of the word is preserved in W2V\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hFhMGpiGwbUR"
   },
   "source": [
    "In this experiment we will be using a huge dataset named as 20 news classification dataset. This data set consists of 20000 messages taken from 20 newsgroups.\n",
    "\n",
    "\n",
    "\t "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L0drLlmqwbUT"
   },
   "source": [
    "### Datasource\n",
    "http://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AunUB3i6wbUT"
   },
   "source": [
    "To get a sense of our data, let us first start by counting the frequencies of the target classes in our news articles in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y53rHt9NwbUU"
   },
   "source": [
    "#### Keywords\n",
    "\n",
    "* Numpy\n",
    "* Collections\n",
    "* Gensim\n",
    "* Bag-of-Words (Word Frequency, Pre-Processing)\n",
    "* Bag-of-Words representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rN0OyDALwbUU"
   },
   "source": [
    "### Setup Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "colab_type": "code",
    "id": "Iak9BpAQEYLf",
    "outputId": "95e534e1-16cc-413f-85ae-e8ec405bf566"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)',\n",
       " 'Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)',\n",
       " 'Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.4)',\n",
       " 'Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (2.0.0)',\n",
       " 'Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)',\n",
       " 'Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.13.13)',\n",
       " 'Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.23.0)',\n",
       " 'Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)',\n",
       " 'Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.3.3)',\n",
       " 'Requirement already satisfied: botocore<1.17.0,>=1.16.13 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.16.13)',\n",
       " 'Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.10.0)',\n",
       " 'Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2020.4.5.1)',\n",
       " 'Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.9)',\n",
       " 'Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)',\n",
       " 'Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)',\n",
       " 'Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.13->boto3->smart-open>=1.2.1->gensim) (0.15.2)',\n",
       " 'Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.13->boto3->smart-open>=1.2.1->gensim) (2.8.1)']"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title Run this cell to complete the setup for this Notebook\n",
    "\n",
    "from IPython import get_ipython\n",
    "ipython = get_ipython()\n",
    "\n",
    "ipython.magic(\"sx wget https://www.dropbox.com/s/ir5kph0ocvibaqm/Setup_W1_D1_Exp1.sh?dl=1\")\n",
    "ipython.magic(\"sx mv Setup_W1_D1_Exp1.sh?dl=1 Setup_W1_D1_Exp1.sh\")\n",
    "ipython.magic(\"sx bash Setup_W1_D1_Exp1.sh -u standard -p pass123\")\n",
    "ipython.magic(\"sx pip3 install gensim\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H9sC9--9wbUq"
   },
   "outputs": [],
   "source": [
    "# Importing required Packages\n",
    "import pickle\n",
    "import re\n",
    "import operator\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import collections\n",
    "import gensim\n",
    "from nltk import ngrams\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "JEhKu_rjwbUt",
    "outputId": "b9a897f0-e98a-486b-b425-622a05e24685"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['talk.politics.mideast', 'rec.autos', 'comp.sys.mac.hardware', 'alt.atheism', 'rec.sport.baseball', 'comp.os.ms-windows.misc', 'rec.sport.hockey', 'sci.crypt', 'sci.med', 'talk.politics.misc', 'rec.motorcycles', 'comp.windows.x', 'comp.graphics', 'comp.sys.ibm.pc.hardware', 'sci.electronics', 'talk.politics.guns', 'sci.space', 'soc.religion.christian', 'misc.forsale', 'talk.religion.misc'])\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "dataset = pickle.load(open('week1_exp1/AIML_DS_NEWSGROUPS_PICKELFILE.pkl','rb'))\n",
    "print(dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "colab_type": "code",
    "id": "ZF9S3GYSwbUx",
    "outputId": "612628b6-74e2-49f1-9afd-e79087770733"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class : count\n",
      "--------------\n",
      "talk.politics.mideast : 940\n",
      "rec.autos : 990\n",
      "comp.sys.mac.hardware : 961\n",
      "alt.atheism : 799\n",
      "rec.sport.baseball : 994\n",
      "comp.os.ms-windows.misc : 985\n",
      "rec.sport.hockey : 999\n",
      "sci.crypt : 991\n",
      "sci.med : 990\n",
      "talk.politics.misc : 775\n",
      "rec.motorcycles : 994\n",
      "comp.windows.x : 980\n",
      "comp.graphics : 973\n",
      "comp.sys.ibm.pc.hardware : 982\n",
      "sci.electronics : 981\n",
      "talk.politics.guns : 910\n",
      "sci.space : 987\n",
      "soc.religion.christian : 997\n",
      "misc.forsale : 972\n",
      "talk.religion.misc : 628\n"
     ]
    }
   ],
   "source": [
    "# Print frequencies of dataset\n",
    "print(\"Class : count\")\n",
    "print(\"--------------\")\n",
    "number_of_documents = 0\n",
    "for key in dataset:\n",
    "    print(key, ':', len(dataset[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gJDnkxkmwbU1"
   },
   "source": [
    "Next, let us split our dataset which consists of 1000 samples per class, into training and test sets. We use 950 samples from each class in the training set, and the remaining 50 in the test set. \n",
    "\n",
    "As a mental exercise you should try reasoning about why is it important to ensure a nearly equal distribution of classes in your training and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u6GK60NEwbU2"
   },
   "outputs": [],
   "source": [
    "train_set = {}\n",
    "test_set = {}\n",
    "\n",
    "# Clean dataset for text encoding issues :- Very useful when dealing with non-unicode characters\n",
    "for key in dataset:\n",
    "    dataset[key] = [[i.decode('utf-8', errors='replace').lower() for i in f] for f in dataset[key]]\n",
    "    \n",
    "# Break dataset into 95-5 split for training and testing\n",
    "n_train = 0\n",
    "n_test = 0\n",
    "for k in dataset:\n",
    "    split = int(0.95*len(dataset[k]))\n",
    "    train_set[k] = dataset[k][0:split]\n",
    "    test_set[k] = dataset[k][split:-1]\n",
    "    n_train += len(train_set[k])\n",
    "    n_test += len(test_set[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "afd4qfAEwbU6"
   },
   "source": [
    "## 1. Bag-of-Words\n",
    "\n",
    "Let us begin our journey into text classification with one of the simplest but most commonly used feature representations for news documents - Bag-of-Words.\n",
    "\n",
    "As you might have realized, machine learning algorithms need good feature representations of different inputs.  Concretely, we would like to represent each news article $D$ in terms of a feature vector $V$, which can be used for classification. Feature vector $V$ is made up of the number of occurences of each word in the vocabulary.\n",
    "\n",
    "Let us begin by counting the number of occurences of every word in the news documents in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OACynazXwbU7"
   },
   "source": [
    "### 1.1 Word frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zWgzZVqYwbU8"
   },
   "source": [
    "### Let us try understanding the kind of words that appear frequently, and those that occur rarely. We now count the frequencies of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9kyqVjvBWLdK"
   },
   "outputs": [],
   "source": [
    "def frequency_words(train_set):\n",
    "  frequency = defaultdict(int)\n",
    "\n",
    "  for key in train_set:\n",
    "      for f in train_set[key]:\n",
    "\n",
    "          # Find all words which consist only of capital and lowercase characters and are between length of 2-9.\n",
    "          # We ignore all special characters such as !.$ and words containing numbers\n",
    "          words = re.findall(r'(\\b[A-Za-z][a-z]{2,9}\\b)', ' '.join(f))\n",
    "\n",
    "          for word in words:\n",
    "              frequency[word] += 1\n",
    "  return frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AdILkwjHwbU9"
   },
   "outputs": [],
   "source": [
    "frequency_of_words = frequency_words(train_set)\n",
    "\n",
    "sorted_words = sorted(frequency_of_words.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print(\"Top-10 most frequent words:\")\n",
    "for word in sorted_words[:10]:\n",
    "    print(word)\n",
    "\n",
    "print('----------------------------')\n",
    "print(\"10 least frequent words:\")\n",
    "for word in sorted_words[-10:-1]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YxZqnO8GwbVB"
   },
   "source": [
    "Next, we attempt to plot a histogram of the counts of various words in descending order. \n",
    "\n",
    "Could you comment about the relationship between the frequency of the most frequent word to the second frequent word? \n",
    "And what about the third most frequent word?\n",
    "\n",
    "(Hint - Check the relative frequencies of the first, second and third most frequent words)\n",
    "\n",
    "(After answering, you can visit https://en.wikipedia.org/wiki/Zipf%27s_law for further Reading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kyWfgN4qwbVC"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(20,10)\n",
    "\n",
    "plt.bar(range(len(sorted_words[:100])), [v for k, v in sorted_words[:100]] , align='center')\n",
    "plt.xticks(range(len(sorted_words[:100])), [k for k, v in sorted_words[:100]])\n",
    "locs, labels = plt.xticks()\n",
    "plt.setp(labels, rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yvcU4VDkwbVE"
   },
   "source": [
    "### 1.2 Pre-processing to remove most and least frequent words\n",
    "\n",
    "We can see that different words appear with different frequencies.\n",
    "\n",
    "The most common words appear in almost all documents. Hence, for a classification task, having information about those words' frequencies does not mater much since they appear frequently in every type of document. To get a good feature representation, we eliminate them since they do not add too much value.\n",
    "\n",
    "Additionally, notice how the least frequent words appear so rarely that they might not be useful either.\n",
    "\n",
    "Let us pre-process our news articles now to remove the most frequent and least frequent words by thresholding their counts: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TrBX6IOSaofH"
   },
   "outputs": [],
   "source": [
    "def cleaning_vocabulary_words(list_of_grams):\n",
    "  valid_words = defaultdict(int)\n",
    "\n",
    "  print('Number of words before preprocessing:', len(list_of_grams))\n",
    "\n",
    "  # Ignore the 25 most frequent words, and the words which appear less than 100 times\n",
    "  ignore_most_frequent = 25\n",
    "  freq_thresh = 100\n",
    "  feature_number = 0\n",
    "  for word, word_frequency in list_of_grams[ignore_most_frequent:]:\n",
    "      if word_frequency > freq_thresh:\n",
    "          valid_words[word] = feature_number\n",
    "          feature_number += 1\n",
    "      elif '_' in word:\n",
    "          valid_words[word] = feature_number\n",
    "          feature_number += 1\n",
    "\n",
    "  print('Number of words after preprocessing:', len(valid_words))\n",
    "\n",
    "  vector_size = len(valid_words)\n",
    "  return valid_words, vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ztxnNsH8bVQV"
   },
   "outputs": [],
   "source": [
    "valid_words, number_of_words = cleaning_vocabulary_words(sorted_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OL7KDD5nwbVG"
   },
   "source": [
    "### 1.3 Bag-of-Words representation\n",
    "\n",
    "The simplest way to represent a document $D$ as a vector $V$ would be to now count the relevant words in the document. \n",
    "\n",
    "For each document, make a vector of the count of each of the words in the vocabulary (excluding the words removed in the previous step - the \"stopwords\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hmPFrRWfwbVH"
   },
   "outputs": [],
   "source": [
    "def convert_to_BoW(dataset, number_of_documents):\n",
    "    bow_representation = np.zeros((number_of_documents, number_of_words))\n",
    "    labels = np.zeros((number_of_documents, 1))\n",
    "    \n",
    "    i = 0\n",
    "    for label, class_name in enumerate(dataset):\n",
    "        \n",
    "        # For each file\n",
    "        for f in dataset[class_name]:\n",
    "            \n",
    "           # Read text from each file and create BoW representations for them.\n",
    "    \n",
    "    return bow_representation, labels\n",
    "\n",
    "# Convert the dataset into their bag of words representation treating train and test separately\n",
    "train_bow_set, train_bow_labels = convert_to_BoW(train_set, n_train)\n",
    "test_bow_set, test_bow_labels = convert_to_BoW(test_set, n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b2L9gwfXwbVI"
   },
   "source": [
    "### 1.4 Document classification using Bag-of-Words\n",
    "\n",
    "For the test documents, use your favorite distance metric (Cosine, Eucilidean, etc.) to find similar news articles from your training set and classify using kNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hGen5XWiwbVK"
   },
   "outputs": [],
   "source": [
    "# Optimized K-NN:- This does the same thing as you've learned but in an optimized manner\n",
    "def dist(train_features, given_feature):\n",
    "    # Calculate euclidea distaces between the training feature set and the given feature\n",
    "    # Try and optimise this calculation using in built numpy functions rather than for loops\n",
    "    return distances\n",
    "\n",
    "''' \n",
    " Optimized K-NN code. This code is the same as what you've already seen, but trades off memory efficency \n",
    " for computational efficency. \n",
    "'''\n",
    "def kNN(k, train_features, train_labels, given_feature):\n",
    "    distances = []\n",
    "    \n",
    "    n = train_features.shape[0]\n",
    "    \n",
    "    # np.tile function repeats the given_feature n times.\n",
    "    given_feature = np.tile(given_feature, (n, 1))\n",
    "    \n",
    "    # Compute distance\n",
    "    distances = dist(train_features, given_feature)\n",
    "    sort_neighbors = np.argsort(distances)\n",
    "    return np.concatenate((distances[sort_neighbors][:k].reshape(-1, 1), train_labels[sort_neighbors][:k].reshape(-1, 1)), axis = 1)\n",
    "\n",
    "def kNN_classify(k, train_features, train_labels, given_feature):\n",
    "    tally = collections.Counter()\n",
    "    tally.update(str(int(nn[1])) for nn in kNN(k, train_features, train_labels, given_feature))\n",
    "    return int(tally.most_common(1)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p1oZ_fMvwbVL"
   },
   "source": [
    "For example, using 3 nearest neighbours, the $0^{th}$ test document is classified as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oUjby7ObwbVN"
   },
   "source": [
    "Computing accuracy for the bag-of-words features on the full test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bjxqR2gUwbVO"
   },
   "outputs": [],
   "source": [
    "accuracy = 0\n",
    "for i, given_feature in enumerate(test_bow_set):\n",
    "    print(\"Progress: {0:.04f}\".format((i+1)/len(test_bow_set)), end=\"\\r\")\n",
    "    predicted_class = kNN_classify(3, train_bow_set, train_bow_labels, given_feature)\n",
    "    if predicted_class == int(test_bow_labels[i]):\n",
    "        accuracy += 1\n",
    "BoW_accuracy = (accuracy / len(test_bow_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FJk9zGFMYe2G"
   },
   "outputs": [],
   "source": [
    "print(BoW_accuracy)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "BoW_Word2Vec_C.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
